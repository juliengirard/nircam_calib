{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1ac475d",
   "metadata": {},
   "source": [
    "# NRC-23 - Image Quality Verification by Filter   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f978551",
   "metadata": {},
   "source": [
    "## Notebook: Create an empirical PSF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a046c63a",
   "metadata": {},
   "source": [
    "**Author**: Matteo Correnti, STScI Scientist II\n",
    "<br>\n",
    "**Created**: November, 2021\n",
    "<br>\n",
    "**Last Updated**: February, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7943ca",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "1. [Introduction](#intro)<br>\n",
    "2. [Setup](#setup)<br>\n",
    "    2.1 [Python imports](#py_imports)<br>\n",
    "    2.2 [Plotting functions imports](#matpl_imports)<br>\n",
    "    2.3 [PSF FWHM dictionary](#psf_fwhm)<br>\n",
    "3. [Import images to analyze](#data)<br>\n",
    "    3.1 [Select Detector/Filter to analyze](#sel_data)<br>\n",
    "    3.2 [Display image](#display_data)<br>\n",
    "    3.3 [Convert image units and apply pixel area map](#convert_data)<br>\n",
    "4. [Create the empirical PSF model](#epsf_intro)<br>\n",
    "    4.1 [Calculate the background](#bkg)<br>\n",
    "    4.2 [Find sources in the image](#find)<br>\n",
    "    4.3 [Select sources](#select)<br>\n",
    "    4.4 [Create catalog of selected sources](#create_cat)<br>\n",
    "    4.5 [Build the empirical PSFs](#build_epsf)<br>\n",
    "    4.6 [Display the empirical PSFs](#display_epsf)<br>\n",
    "5. [Create a single or grid of empirical PSFs](#epsf_intro2)<br>\n",
    "    5.1 [Count stats in N x N grid](#count_stars)<br>\n",
    "    5.2 [Build effective PSF (single or grid)](#epsf_grid)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51831ddb",
   "metadata": {},
   "source": [
    "1.<font color='white'>-</font>Introduction <a class=\"anchor\" id=\"intro\"></a>\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2224cc0c",
   "metadata": {},
   "source": [
    "This notebook shows how to create an empirical PSF or a grid of empirical PSFs. The choice between a single or a grid is dictated by the number of good PSF stars that will be observed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403035df",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <h3><u><b>Warning</b></u></h3>\n",
    "\n",
    "This notebook has the primary goal to showcase how it is possible to create an empirical PSF using PhotUtils (in particular the function EPSFBuilider). It is important to note that an accurate empirical PSF (or a grid of empirical PSF) can be derived from a single image **only** if we have a significant number of sources in the image.  \n",
    "<div >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e189eb39",
   "metadata": {},
   "source": [
    "2.<font color='white'>-</font>Setup <a class=\"anchor\" id=\"setup\"></a>\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769cccb8",
   "metadata": {},
   "source": [
    "In this section we import all the necessary Python packages and we define some plotting parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599f7d8d",
   "metadata": {},
   "source": [
    "### 2.1<font color='white'>-</font>Python imports<a class=\"anchor\" id=\"py_imports\"></a> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b8a089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import glob as glob\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "\n",
    "from astropy.io import fits\n",
    "from astropy.visualization import simple_norm\n",
    "from astropy.table import Table\n",
    "from astropy.nddata import NDData\n",
    "from astropy.stats import sigma_clipped_stats, SigmaClip\n",
    "\n",
    "from photutils.background import MMMBackground, MADStdBackgroundRMS, Background2D\n",
    "from photutils.detection import DAOStarFinder\n",
    "from photutils.psf import extract_stars\n",
    "from photutils import EPSFBuilder, GriddedPSFModel\n",
    "\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda3d894",
   "metadata": {},
   "source": [
    "### 2.2<font color='white'>-</font>Plotting function imports<a class=\"anchor\" id=\"matpl_imports\"></a> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8a5694",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import style, pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "plt.rcParams['image.cmap'] = 'viridis'\n",
    "plt.rcParams['image.origin'] = 'lower'\n",
    "plt.rcParams['axes.titlesize'] = plt.rcParams['axes.labelsize'] = 30\n",
    "plt.rcParams['xtick.labelsize'] = plt.rcParams['ytick.labelsize'] = 20\n",
    "\n",
    "font1 = {'family': 'helvetica', 'color': 'black', 'weight': 'normal', 'size': '12'}\n",
    "font2 = {'family': 'helvetica', 'color': 'black', 'weight': 'normal', 'size': '20'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cb8f52",
   "metadata": {},
   "source": [
    "### 2.3<font color='white'>-</font>PSF FWHM dictionary<a class=\"anchor\" id=\"psf_fwhm\"></a> ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641a4fc9",
   "metadata": {},
   "source": [
    "The dictionary contains the NIRCam point spread function (PSF) FWHM, from the [NIRCam Point Spread Function](https://jwst-docs.stsci.edu/near-infrared-camera/nircam-predicted-performance/nircam-point-spread-functions) JDox page. The FWHM are calculated from the analysis of the expected NIRCam PSFs simulated with [WebbPSF](https://www.stsci.edu/jwst/science-planning/proposal-planning-toolbox/psf-simulation-tool). \n",
    "\n",
    "FWHM is used in the finding script to provide a first order discrimination between sources and spurious detections\n",
    "\n",
    "**Note**: this dictionary need to be updated once the values for the FWHM will be available for each detectors during commissioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796417c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = ['F070W', 'F090W', 'F115W', 'F140M', 'F150W2', 'F150W', 'F162M', 'F164N', 'F182M',\n",
    "           'F187N', 'F200W', 'F210M', 'F212N', 'F250M', 'F277W', 'F300M', 'F322W2', 'F323N',\n",
    "           'F335M', 'F356W', 'F360M', 'F405N', 'F410M', 'F430M', 'F444W', 'F460M', 'F466N', 'F470N', 'F480M']\n",
    "\n",
    "psf_fwhm = [0.987, 1.103, 1.298, 1.553, 1.628, 1.770, 1.801, 1.494, 1.990, 2.060, 2.141, 2.304, 2.341, 1.340,\n",
    "            1.444, 1.585, 1.547, 1.711, 1.760, 1.830, 1.901, 2.165, 2.179, 2.300, 2.302, 2.459, 2.507, 2.535, 2.574]\n",
    "\n",
    "dict_utils = {filters[i]: {'psf fwhm': psf_fwhm[i]} for i in range(len(filters))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b2b238",
   "metadata": {},
   "source": [
    "3.<font color='white'>-</font>Import images to analyze<a class=\"anchor\" id=\"data\"></a>\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8122f005",
   "metadata": {},
   "source": [
    "We load all the images and we create a dictionary that contains all of them, divided by detectors and filters. This is useful to check which detectors and filters are available and to perform the analysis presented in this notebook on a detector/filter base. \n",
    "\n",
    "We retrieve the NIRCam detector and filter from the image header. Note that for the LW channels, we transform the detector name derived from the header (**NRCBLONG**) to **NRCB5** for consitency with other the notebooks created for NRC-23. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3981620d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_images = {'NRCA1': {}, 'NRCA2': {}, 'NRCA3': {}, 'NRCA4': {}, 'NRCA5': {},\n",
    "               'NRCB1': {}, 'NRCB2': {}, 'NRCB3': {}, 'NRCB4': {}, 'NRCB5': {}}\n",
    "\n",
    "dict_filter_short = {}\n",
    "dict_filter_long = {}\n",
    "\n",
    "ff_short = []\n",
    "det_short = []\n",
    "det_long = []\n",
    "ff_long = []\n",
    "detlist_short = []\n",
    "detlist_long = []\n",
    "filtlist_short = []\n",
    "filtlist_long = []\n",
    "\n",
    "images_dir = '../Simulation/Pipeline_Outputs/Level2_Outputs'\n",
    "images = sorted(glob.glob(os.path.join(images_dir, \"*cal.fits\")))\n",
    "\n",
    "for image in images:\n",
    "\n",
    "    im = fits.open(image)\n",
    "    f = im[0].header['FILTER']\n",
    "    d = im[0].header['DETECTOR']\n",
    "    p = im[0].header['PUPIL']\n",
    "\n",
    "    if d == 'NRCBLONG':\n",
    "        d = 'NRCB5'\n",
    "    elif d == 'NRCALONG':\n",
    "        d = 'NRCA5'\n",
    "    else:\n",
    "        d = d\n",
    "    \n",
    "    if p == 'CLEAR':\n",
    "        f = f\n",
    "    else:\n",
    "        f = p\n",
    "    \n",
    "    wv = float(f[1:3])\n",
    "\n",
    "    if wv > 24:         \n",
    "        ff_long.append(f)\n",
    "        det_long.append(d)\n",
    "\n",
    "    else:\n",
    "        ff_short.append(f)\n",
    "        det_short.append(d)   \n",
    "\n",
    "    detlist_short = sorted(list(dict.fromkeys(det_short)))\n",
    "    detlist_long = sorted(list(dict.fromkeys(det_long)))\n",
    "\n",
    "    unique_list_filters_short = []\n",
    "    unique_list_filters_long = []\n",
    "\n",
    "    for x in ff_short:\n",
    "\n",
    "        if x not in unique_list_filters_short:\n",
    "\n",
    "            dict_filter_short.setdefault(x, {})\n",
    "                 \n",
    "    for x in ff_long:\n",
    "        if x not in unique_list_filters_long:\n",
    "            dict_filter_long.setdefault(x, {})   \n",
    "            \n",
    "    for d_s in detlist_short:\n",
    "        dict_images[d_s] = copy.deepcopy(dict_filter_short)\n",
    "\n",
    "    for d_l in detlist_long:\n",
    "        dict_images[d_l] = copy.deepcopy(dict_filter_long)\n",
    "\n",
    "    filtlist_short = sorted(list(dict.fromkeys(dict_filter_short)))\n",
    "    filtlist_long = sorted(list(dict.fromkeys(dict_filter_long)))\n",
    "\n",
    "print(\"Available Detectors for SW channel:\", detlist_short)\n",
    "print(\"Available Detectors for LW channel:\", detlist_long)\n",
    "print(\"Available SW Filters:\", filtlist_short)\n",
    "print(\"Available LW Filters:\", filtlist_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b533c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in images:\n",
    "    \n",
    "    im = fits.open(image)\n",
    "    f = im[0].header['FILTER']\n",
    "    d = im[0].header['DETECTOR']\n",
    "    p = im[0].header['PUPIL']\n",
    "\n",
    "    if d == 'NRCBLONG':\n",
    "        d = 'NRCB5'\n",
    "    elif d == 'NRCALONG':\n",
    "        d = 'NRCA5'\n",
    "    else:\n",
    "        d = d\n",
    "    \n",
    "    if p == 'CLEAR':\n",
    "        f = f\n",
    "    else:\n",
    "        f = p\n",
    "\n",
    "    if len(dict_images[d][f]) == 0:\n",
    "        dict_images[d][f] = {'images': [image]}\n",
    "    else:\n",
    "        dict_images[d][f]['images'].append(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21648ffe",
   "metadata": {},
   "source": [
    "### 3.1<font color='white'>-</font>Select detector/filter to analyze<a class=\"anchor\" id=\"sel_data\"></a> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119a4d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "det = 'NRCB1'\n",
    "filt = 'F200W'\n",
    "\n",
    "num_images = len(dict_images[det][filt]['images'])\n",
    "images_original = dict_images[det][filt]['images']\n",
    "\n",
    "print('Number of images for detector {}, filter {}:'.format(det, filt), num_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ca3858",
   "metadata": {},
   "source": [
    "### 3.2<font color='white'>-</font>Display the image<a class=\"anchor\" id=\"display_data\"></a> ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83b5b76",
   "metadata": {},
   "source": [
    "To check that our images do not present artifacts and can be used in the analysis, we display them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8711bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(images_original) > 2:\n",
    "\n",
    "    nn = int(np.sqrt(len(images_original)))\n",
    "    figsize = (12, 12)\n",
    "    fig, ax = plt.subplots(nn, nn, figsize=figsize)\n",
    "\n",
    "    for ix in range(nn):\n",
    "        for iy in range(nn):\n",
    "            \n",
    "            i = ix * nn + iy\n",
    "            \n",
    "            im = fits.open(dict_images[det][filt]['images'][i])\n",
    "            data_sb = im[1].data\n",
    "            \n",
    "            ax[nn - 1 - ix, iy].set_xlabel('X [px]', fontsize=15)\n",
    "            ax[nn - 1 - ix, iy].set_ylabel('Y [px]', fontsize=15)\n",
    "            \n",
    "            norm = simple_norm(data_sb, 'sqrt', percent=99.)\n",
    "            ax[nn - 1 - ix, iy].set_title(det + ' - ' + filt +  ' - image' + str(i+1), fontsize=20)\n",
    "            ax[nn - 1 - ix, iy].imshow(data_sb, norm=norm, cmap='Greys')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "else:\n",
    "    \n",
    "    plt.figure(figsize = (14, 14))\n",
    "    nn = 2 \n",
    "    for i in range(nn):\n",
    "        ax = plt.subplot(1, nn, i + 1)\n",
    "        \n",
    "        im = fits.open(dict_images[det][filt]['images'][i])\n",
    "        data_sb = im[1].data\n",
    "        \n",
    "        ax.set_xlabel('X [px]')\n",
    "        ax.set_ylabel('Y [px]')\n",
    "        ax.set_title(det + ' - ' + filt +  ' - image' + str(i+1), fontsize=20)\n",
    "        norm = simple_norm(data_sb, 'sqrt', percent=99.)\n",
    "        ax.imshow(data_sb, norm=norm, cmap='Greys')\n",
    "       \n",
    "        plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887b93b8",
   "metadata": {},
   "source": [
    "### 3.3<font color='white'>-</font>Convert image units and apply pixel area map<a class=\"anchor\" id=\"convert_data\"></a> ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05c6086",
   "metadata": {},
   "source": [
    "The unit of the Level-2 and Level-3 Images from the pipeline is MJy/sr (hence a surface brightness). The actual unit of the image can be checked from the header keyword **BUNIT**. The scalar conversion constant is copied to the header keyword **PHOTMJSR**, which gives the conversion from DN/s to megaJy/steradian. For our analysis we revert back to DN/s. It is possible to revert back to DN/s setting `unit = True` in the function below.\n",
    "\n",
    "For images that have not been transformed into a distortion-free frame (i.e. not drizzled), a correction must be applied to account for the different on-sky pixel size across the field of view. A pixel area map (PAM), which is an image where each pixel value describes that pixel's area on the sky relative to the native plate scale, is used for this correction. In the stage 2 of the JWST pipeline, the PAM is copied into an image extension called **AREA** in the science data product. To apply the PAM correction, set the parameter `distorted = True` in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c742bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pam_image(image, units=True, distorted=True):\n",
    "    im = fits.open(image)\n",
    "    data_sb = im[1].data\n",
    "    imh = im[1].header\n",
    "    dq = im[3].data\n",
    "    \n",
    "    if units:\n",
    "        \n",
    "        print('Converting units from {0} to DN/s').format(imh['BUNIT'])\n",
    "        data = data_sb / imh['PHOTMJSR']\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        data = data_sb\n",
    "    \n",
    "    zero_mask = np.where(data == 0,0,1)\n",
    "    nan_mask  = np.where(np.isnan(data),0,1)\n",
    "    zero_mask = nan_mask * zero_mask\n",
    "    \n",
    "    nan_mask = np.where(zero_mask == 0,True,False)\n",
    "    data_mask = nan_mask\n",
    "    \n",
    "    if distorted:\n",
    "        print('Analyzing Level-2 image - *cal.fits and correcting for PAM')\n",
    "        area = im[4].data\n",
    "        data_corrected = data * area\n",
    "    else:\n",
    "        print('Analyzing Level-3 image - *i2d.fits or not using PAM correction')\n",
    "        data_corrected = data\n",
    "    \n",
    "    return data_corrected, data_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8634ab",
   "metadata": {},
   "source": [
    "4.<font color='white'>-</font>Create the empirical PSF model<a class=\"anchor\" id=\"epsf_intro\"></a>\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f164553",
   "metadata": {},
   "source": [
    "More information on the PhotUtils Effective PSF can be found [here](https://photutils.readthedocs.io/en/stable/epsf.html).\n",
    "\n",
    "The process of creating an effective PSF can be summarized as follows:\n",
    "\n",
    "* Find the stars in the image.\n",
    "* Select the stars we want to use for building the effective PSF. \n",
    "* Build the effective PSF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe872ffb",
   "metadata": {},
   "source": [
    "### 4.1<font color='white'>-</font>Calculate the background<a class=\"anchor\" id=\"bkg\"></a> ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2832704",
   "metadata": {},
   "source": [
    "We can adopt as Background estimator the function [MMMBackground](https://photutils.readthedocs.io/en/stable/api/photutils.background.MMMBackground.html#photutils.background.MMMBackground), which calculates the background in an array using the DAOPHOT MMM algorithm, on the whole image (The background is calculated using a mode estimator of the form `(3 * median) - (2 * mean)`). \n",
    "\n",
    "However, when dealing with a variable background and/or the need to mask the regions where we have no data (for example, if we are analyzing an image with all the 4 NIRCam SW detectors, i.e. containing the chip gaps), we can set `var_bkg = True` and use a more complex algorithm that takes into account those issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e16eb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bkg(data, mask, var_bkg=False):\n",
    "    \n",
    "    bkgrms = MADStdBackgroundRMS()\n",
    "    mmm_bkg = MMMBackground()\n",
    "\n",
    "    if var_bkg:\n",
    "        print('Using 2D Background')\n",
    "        sigma_clip = SigmaClip(sigma=3.)\n",
    "        \n",
    "\n",
    "        bkg = Background2D(data, (100, 100), filter_size=(3, 3), sigma_clip=sigma_clip, bkg_estimator=mmm_bkg,\n",
    "                           coverage_mask=mask, fill_value=0.0)\n",
    "\n",
    "        data_bkgsub = data.copy()\n",
    "        data_bkgsub = data_bkgsub - bkg.background\n",
    "\n",
    "        median = bkg.background_median\n",
    "        std = bkg.background_rms_median        \n",
    "        print('Background median and rms using Background 2D:', median, std)\n",
    "\n",
    "\n",
    "    else:\n",
    "\n",
    "        std = bkgrms(data)\n",
    "        bkg = mmm_bkg(data)\n",
    "        print('Background median and rms:', bkg, std)\n",
    "        data_bkgsub = data.copy()\n",
    "        data_bkgsub -= bkg\n",
    "\n",
    "    return data_bkgsub, std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16156114",
   "metadata": {},
   "source": [
    "### 4.2<font color='white'>-</font>Find sources in the image<a class=\"anchor\" id=\"find\"></a> ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875e0a88",
   "metadata": {},
   "source": [
    "To find sources in the image, we use the [DAOStarFinder](https://photutils.readthedocs.io/en/stable/api/photutils.detection.DAOStarFinder.html) function. \n",
    "\n",
    "[DAOStarFinder](https://photutils.readthedocs.io/en/stable/api/photutils.detection.DAOStarFinder.html) detects stars in an image using the DAOFIND ([Stetson 1987](https://ui.adsabs.harvard.edu/abs/1987PASP...99..191S/abstract)) algorithm. DAOFIND searches images for local density maxima that have a peak amplitude greater than `threshold` (approximately; threshold is applied to a convolved image) and have a size and shape similar to the defined 2D Gaussian kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9a19f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_stars(image, det='NRCA1', filt='F070W', threshold=3, var_bkg=False):\n",
    "    \n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    threshold : float \n",
    "        The absolute image value above which to select sources.\n",
    "    \n",
    "    fwhm : float\n",
    "        The full-width half-maximum (FWHM) of the major axis of the Gaussian kernel in units of pixels.\n",
    "        \n",
    "    var_bkg : bool\n",
    "        Use Background2D (see description above)\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    print('Finding stars --- Detector: {d}, Filter: {f}'.format(f=filt, d=det))\n",
    "    \n",
    "    sigma_psf = dict_utils[filt]['psf fwhm']\n",
    "\n",
    "    print('FWHM for the filter {f}:'.format(f=filt), sigma_psf, \"px\")\n",
    "    \n",
    "    print('Converting to MJy/sr to DN/s and applying Pixel Area Map (if needed)')\n",
    "    \n",
    "    data_converted, data_mask = convert_pam_image(image, distorted=True)\n",
    "    \n",
    "    data_bkgsub, std = calc_bkg(data_converted, data_mask, var_bkg=var_bkg)\n",
    "    \n",
    "    daofind = DAOStarFinder(threshold=threshold * std, fwhm=sigma_psf)\n",
    "    found_stars = daofind(data_bkgsub, mask=data_mask)\n",
    "    \n",
    "    print('')\n",
    "    print('Number of sources found in the image:', len(found_stars))\n",
    "    print('-------------------------------------')\n",
    "    print('')\n",
    "    \n",
    "    return found_stars, data_bkgsub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284a76fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.perf_counter()\n",
    "\n",
    "found_stars_tot = []\n",
    "data_bkgsub_tot = []\n",
    "\n",
    "\n",
    "\n",
    "for i, image in enumerate(images_original):\n",
    "    \n",
    "    print('Working on image: {}'.format(i + 1))\n",
    "    print('')\n",
    "    found_stars, data_bkgsub = find_stars(image, det=det, filt=filt, threshold=5, var_bkg=True)\n",
    "    \n",
    "    found_stars_tot.append(found_stars)\n",
    "    data_bkgsub_tot.append(data_bkgsub)\n",
    "\n",
    "toc = time.perf_counter()\n",
    "print(\"Elapsed Time for finding stars:\", toc - tic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87a08af",
   "metadata": {},
   "source": [
    "### 4.3<font color='white'>-</font>Select sources<a class=\"anchor\" id=\"select\"></a> ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5e7908",
   "metadata": {},
   "source": [
    "We can adopt different methods to select sources we want to use to build an effective PSF. Here, we select objects applying a brightness cut (we do not want to include objects that are too faint) and using the `roundness2` and `sharpness` parameters provided in the [DAOStarFinder](https://photutils.readthedocs.io/en/stable/api/photutils.detection.DAOStarFinder.html) output catalog.\n",
    "\n",
    "`roundness2` measures the ratio of the difference in the height of the best fitting Gaussian function in x minus the best fitting Gaussian function in y, divided by the average of the best fitting Gaussian functions in x and y.\n",
    "\n",
    "`sharpness` measures the ratio of the difference between the height of the central pixel and the mean of the surrounding non-bad pixels in the convolved image, to the height of the best fitting Gaussian function at that point.\n",
    "\n",
    "We derive the cut from 1 image and we verify that they are appropriate for all the other images.\n",
    "\n",
    "**Note**: when we derive the selection cuts for a particular detector/filter, they are stored in a file at the end of section 4.3. Hence, below we chek if the file exists so we can skip the selection from the images.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f9bcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_dir = 'CUT_FOR_ePSF/'\n",
    "\n",
    "if not os.path.exists(dict_dir):\n",
    "    os.makedirs(dict_dir)\n",
    "\n",
    "dict_filename = 'epsf_selection_cuts_{}_{}.pkl'.format(det, filt)\n",
    "\n",
    "if os.path.exists(os.path.join(dict_dir, dict_filename)):\n",
    "    \n",
    "    with open(os.path.join(dict_dir, dict_filename), 'rb') as handle:\n",
    "        \n",
    "    \n",
    "        dict_cut_values = pickle.load(handle)\n",
    "        print('Load dictionary with selection cuts for detector {} - filter {}'.format(det, filt))\n",
    "        print('Skip to section 4.4')\n",
    "\n",
    "else:\n",
    "    print('The selection cuts have not been created yet for detector {} - filter {}'.format(det, filt))\n",
    "    print('Determine the parameters from the plots below')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb173ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.clf()\n",
    "\n",
    "found_stars_test = found_stars_tot[0]\n",
    "\n",
    "ax1 = plt.subplot(2, 1, 1)\n",
    "\n",
    "plt.title('Sharpness and Roundness plots for image 1')\n",
    "\n",
    "ax1.set_xlabel('mag', fontdict=font2)\n",
    "ax1.set_ylabel('sharpness', fontdict=font2)\n",
    "\n",
    "xlim0 = np.min(found_stars_test['mag']) - 0.25\n",
    "xlim1 = np.max(found_stars_test['mag']) + 0.25\n",
    "ylim0 = np.min(found_stars_test['sharpness']) - 0.15\n",
    "ylim1 = np.max(found_stars_test['sharpness']) + 0.15\n",
    "\n",
    "ax1.set_xlim(xlim0, xlim1)\n",
    "ax1.set_ylim(ylim0, ylim1)\n",
    "\n",
    "ax1.xaxis.set_major_locator(ticker.AutoLocator())\n",
    "ax1.xaxis.set_minor_locator(ticker.AutoMinorLocator())\n",
    "ax1.yaxis.set_major_locator(ticker.AutoLocator())\n",
    "ax1.yaxis.set_minor_locator(ticker.AutoMinorLocator())\n",
    "\n",
    "ax1.scatter(found_stars_test['mag'], found_stars_test['sharpness'], s=10, color='k')\n",
    "\n",
    "ax2 = plt.subplot(2, 1, 2)\n",
    "\n",
    "ax2.set_xlabel('mag', fontdict=font2)\n",
    "ax2.set_ylabel('roundness', fontdict=font2)\n",
    "\n",
    "ylim0 = np.min(found_stars['roundness2']) - 0.25\n",
    "ylim1 = np.max(found_stars['roundness2']) - 0.25\n",
    "\n",
    "ax2.set_xlim(xlim0, xlim1)\n",
    "ax2.set_ylim(ylim0, ylim1)\n",
    "\n",
    "ax2.xaxis.set_major_locator(ticker.AutoLocator())\n",
    "ax2.xaxis.set_minor_locator(ticker.AutoMinorLocator())\n",
    "ax2.yaxis.set_major_locator(ticker.AutoLocator())\n",
    "ax2.yaxis.set_minor_locator(ticker.AutoMinorLocator())\n",
    "\n",
    "ax2.scatter(found_stars_test['mag'], found_stars_test['roundness2'], s=10, color='k')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f8f0b4",
   "metadata": {},
   "source": [
    "**Note**: we need to record the values for the selected limits (expectation is that they will not change for the different images of the same detector/filter combination). We create a dictionary to store the values for the different images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a11ddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_cut_values = {}\n",
    "for i in np.arange(num_images):\n",
    "    j = str(i+1)\n",
    "    dict_cut_values['image '+j] = {'sh inf': 0, \n",
    "                   'sh sup': 0,\n",
    "                   'mag lim': 0,\n",
    "                   'round inf': 0,\n",
    "                   'round sup': 0}\n",
    "dict_cut_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544597bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.clf()\n",
    "\n",
    "j = 3\n",
    "\n",
    "found_stars_test = found_stars_tot[j]\n",
    "\n",
    "num = str(j+1)\n",
    "\n",
    "ax1 = plt.subplot(2, 1, 1)\n",
    "\n",
    "plt.title('Sharpness and Roundness plots for image'+ num+' - selection cuts')\n",
    "\n",
    "ax1.set_xlabel('mag', fontdict=font2)\n",
    "ax1.set_ylabel('sharpness', fontdict=font2)\n",
    "\n",
    "xlim0 = np.min(found_stars_test['mag']) - 0.25\n",
    "xlim1 = np.max(found_stars_test['mag']) + 0.25\n",
    "ylim0 = np.min(found_stars_test['sharpness']) - 0.15\n",
    "ylim1 = np.max(found_stars_test['sharpness']) + 0.15\n",
    "\n",
    "ax1.set_xlim(xlim0, xlim1)\n",
    "ax1.set_ylim(ylim0, ylim1)\n",
    "\n",
    "ax1.xaxis.set_major_locator(ticker.AutoLocator())\n",
    "ax1.xaxis.set_minor_locator(ticker.AutoMinorLocator())\n",
    "ax1.yaxis.set_major_locator(ticker.AutoLocator())\n",
    "ax1.yaxis.set_minor_locator(ticker.AutoMinorLocator())\n",
    "\n",
    "ax1.scatter(found_stars_test['mag'], found_stars_test['sharpness'], s=10, color='k')\n",
    "\n",
    "sh_inf = 0.57\n",
    "sh_sup = 0.70\n",
    "mag_lim = -2.5\n",
    "\n",
    "\n",
    "dict_cut_values['image '+num]['sh inf'] = sh_inf\n",
    "dict_cut_values['image '+num]['sh sup'] = sh_sup\n",
    "dict_cut_values['image '+num]['mag lim'] = mag_lim\n",
    "\n",
    "ax1.plot([xlim0, xlim1], [sh_sup, sh_sup], color='r', lw=3, ls='--')\n",
    "ax1.plot([xlim0, xlim1], [sh_inf, sh_inf], color='r', lw=3, ls='--')\n",
    "ax1.plot([mag_lim, mag_lim], [ylim0, ylim1], color='r', lw=3, ls='--')\n",
    "\n",
    "ax2 = plt.subplot(2, 1, 2)\n",
    "\n",
    "ax2.set_xlabel('mag', fontdict=font2)\n",
    "ax2.set_ylabel('roundness', fontdict=font2)\n",
    "\n",
    "ylim0 = np.min(found_stars['roundness2']) - 0.25\n",
    "ylim1 = np.max(found_stars['roundness2']) - 0.25\n",
    "\n",
    "ax2.set_xlim(xlim0, xlim1)\n",
    "ax2.set_ylim(ylim0, ylim1)\n",
    "\n",
    "ax2.xaxis.set_major_locator(ticker.AutoLocator())\n",
    "ax2.xaxis.set_minor_locator(ticker.AutoMinorLocator())\n",
    "ax2.yaxis.set_major_locator(ticker.AutoLocator())\n",
    "ax2.yaxis.set_minor_locator(ticker.AutoMinorLocator())\n",
    "\n",
    "round_inf = -0.30\n",
    "round_sup = 0.30\n",
    "\n",
    "dict_cut_values['image '+num]['round inf'] = round_inf\n",
    "dict_cut_values['image '+num]['round sup'] = round_sup\n",
    "\n",
    "\n",
    "ax2.scatter(found_stars_test['mag'], found_stars_test['roundness2'], s=10, color='k')\n",
    "\n",
    "ax2.plot([xlim0, xlim1], [round_sup, round_sup], color='r', lw=3, ls='--')\n",
    "ax2.plot([xlim0, xlim1], [round_inf, round_inf], color='r', lw=3, ls='--')\n",
    "ax2.plot([mag_lim, mag_lim], [ylim0, ylim1], color='r', lw=3, ls='--')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c66edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_cut_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a51043",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_filename = 'epsf_selection_cuts_{}_{}.pkl'.format(det, filt)\n",
    "\n",
    "with open(os.path.join(dict_dir, dict_filename), 'wb') as handle:\n",
    "    pickle.dump(dict_cut_values, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762450eb",
   "metadata": {},
   "source": [
    "### 4.4<font color='white'>-</font>Create catalog of selected sources<a class=\"anchor\" id=\"create_cat\"></a> ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeb9132",
   "metadata": {},
   "source": [
    "We can also include a separation criteria if we want to retain in the final catalog only the stars that are well isolated. In particular, we can select only the stars that do not have a neighbour closer than X pixel, where X is a parameter that can be set manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a2d160",
   "metadata": {},
   "outputs": [],
   "source": [
    "found_stars_sel_tot = []\n",
    "found_stars_sel_dist_tot = []\n",
    "\n",
    "for i, found_stars in enumerate(found_stars_tot):\n",
    "    \n",
    "    j = str(i+1)\n",
    "    \n",
    "    mask = ((found_stars['mag'] < dict_cut_values['image '+j]['mag lim']) & \n",
    "            (found_stars['roundness2'] > dict_cut_values['image '+j]['round inf']) \n",
    "            & (found_stars['roundness2'] < dict_cut_values['image '+j]['round sup']) \n",
    "            & (found_stars['sharpness'] > dict_cut_values['image '+j]['sh inf']) \n",
    "            & (found_stars['sharpness'] < dict_cut_values['image '+j]['sh sup']))\n",
    "\n",
    "    found_stars_sel = found_stars[mask]\n",
    "\n",
    "    print('Number of stars selected to build ePSF for image {}:'.format(i+1), len(found_stars_sel))\n",
    "\n",
    "    # if we include the separation criteria:\n",
    "\n",
    "    d = []\n",
    "\n",
    "    # we do not want any stars in a 10 px radius. \n",
    "\n",
    "    min_sep = 10\n",
    "\n",
    "    x_tot = found_stars['xcentroid']\n",
    "    y_tot = found_stars['ycentroid']\n",
    "\n",
    "    for xx, yy in zip(found_stars_sel['xcentroid'], found_stars_sel['ycentroid']):\n",
    "\n",
    "        sep = []\n",
    "        dist = np.sqrt((x_tot - xx)**2 + (y_tot - yy)**2)\n",
    "        sep = np.sort(dist)[1:2][0]\n",
    "        d.append(sep)\n",
    "\n",
    "    found_stars_sel['min distance'] = d\n",
    "    mask_dist = (found_stars_sel['min distance'] > min_sep)\n",
    "\n",
    "    found_stars_sel_dist = found_stars_sel[mask_dist]\n",
    "\n",
    "    print('Number of stars selected to build ePSF including \"mimimum distance closest neighbour\" selection for image {}:'.format(i+1), len(found_stars_sel_dist))\n",
    "    print('--------------------------')\n",
    "    print('')\n",
    "    found_stars_sel_tot.append(found_stars_sel)\n",
    "    found_stars_sel_dist_tot.append(found_stars_sel_dist)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebec0c2c",
   "metadata": {},
   "source": [
    "### 4.5<font color='white'>-</font>Build the empirical PSFs<a class=\"anchor\" id=\"build_epsf\"></a> ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decb45a3",
   "metadata": {},
   "source": [
    "We Build the effective PSF using [EPSBuilder](https://photutils.readthedocs.io/en/stable/api/photutils.psf.EPSFBuilder.html#photutils.psf.EPSFBuilder) function.\n",
    "\n",
    "First, we exclude the objects for which the bounding box exceed the detector edge. Then, we extract cutouts of the stars using the [extract_stars()](https://photutils.readthedocs.io/en/stable/api/photutils.psf.extract_stars.html#photutils.psf.extract_stars) function. The size of the cutout is determined by the parameter `size` in our function *build_epsf*. Once we have the object containing the cutouts of our selected stars, we can build our ePSF using [EPSFBuilder](https://photutils.readthedocs.io/en/stable/api/photutils.psf.EPSFBuilder.html#photutils.psf.EPSFBuilder) class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8ba863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_epsf(data, table, det='NRCA1', filt='F070W', size=11, oversample=4, iters=10):\n",
    "    \n",
    "    hsize = (size - 1) / 2\n",
    "    \n",
    "    x = table['xcentroid']\n",
    "    y = table['ycentroid']\n",
    "    \n",
    "    pos_mask = ((x > hsize) & (x < (data.shape[1] - 1 - hsize)) & (y > hsize) & (y < (data.shape[0] - 1 - hsize)))\n",
    "\n",
    "    stars_tbl = Table()\n",
    "    stars_tbl['x'] = x[pos_mask]\n",
    "    stars_tbl['y'] = y[pos_mask]\n",
    "    \n",
    "    nddata = NDData(data=data)\n",
    "    stars = extract_stars(nddata, stars_tbl, size=size)\n",
    "\n",
    "    print('Creating ePSF --- Detector {d}, filter {f}'.format(f=filt, d=det))\n",
    "    print('---------------')\n",
    "\n",
    "    epsf_builder = EPSFBuilder(oversampling=oversample, maxiters=iters, progress_bar=True)\n",
    "\n",
    "    epsf, fitted_stars = epsf_builder(stars)        \n",
    "    \n",
    "    return epsf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51001729",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsf_tot = []\n",
    "\n",
    "size = 41\n",
    "num_psf = 1\n",
    "oversample = 4\n",
    "\n",
    "distorted = True\n",
    "\n",
    "if distorted:\n",
    "    epsf_dir = 'ePSF_MODELS/Distorted/Single_PSF/Fov{}px_numPSFs{}_oversample{}'.format(size, num_psf, oversample)\n",
    "\n",
    "    if not os.path.exists(epsf_dir):\n",
    "        os.makedirs(epsf_dir)\n",
    "else:\n",
    "    epsf_dir = 'ePSF_MODELS/Undistorted/Single_PSF/Fov{}px_numPSFs{}_oversample{}'.format(size, num_psf, oversample)\n",
    "    \n",
    "    if not os.path.exists(epsf_dir):\n",
    "        os.makedirs(epsf_dir)\n",
    "        \n",
    "save_epsf = True\n",
    "\n",
    "for i, (data, table) in enumerate(zip(data_bkgsub_tot, found_stars_sel_tot)):\n",
    "    \n",
    "    print('----------------')\n",
    "    print('Working on image {}'.format(str(i+1)))\n",
    "    print('----------------')\n",
    "    print('')\n",
    "    \n",
    "    epsf = build_epsf(data, table=table, det=det, filt=filt, size=size, oversample=oversample, iters=3)\n",
    "    \n",
    "    if save_epsf:\n",
    "        hdu = fits.PrimaryHDU(epsf.data)\n",
    "        hdul = fits.HDUList([hdu])\n",
    "        epsf_name = 'ePSF_{}_{}_fov{}px_image_{}.fits'.format(det, filt, size, str(i+1))\n",
    "        hdul.writeto(os.path.join(epsf_dir, epsf_name), overwrite = True)\n",
    "    \n",
    "    epsf_tot.append(epsf)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5ef1ab",
   "metadata": {},
   "source": [
    "### 4.6<font color='white'>-</font>Display the emprirical PSFs<a class=\"anchor\" id=\"display_epsf\"></a> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426edec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(images_original) > 2:\n",
    "\n",
    "    nn = int(np.sqrt(len(images_original)))\n",
    "    figsize = (12, 12)\n",
    "    fig, ax = plt.subplots(nn, nn, figsize=figsize)\n",
    "\n",
    "    for ix in range(nn):\n",
    "        for iy in range(nn):\n",
    "            \n",
    "            i = ix * nn + iy\n",
    "            \n",
    "            epsf = epsf_tot[i].data\n",
    "            \n",
    "            ax[nn - 1 - ix, iy].set_xlabel('X [px]', fontsize=15)\n",
    "            ax[nn - 1 - ix, iy].set_ylabel('Y [px]', fontsize=15)\n",
    "            \n",
    "            norm = simple_norm(epsf, 'sqrt', percent=99.)\n",
    "            ax[nn - 1 - ix, iy].set_title(det + ' - ' + filt +  ' - image' + str(i+1), fontsize=20)\n",
    "            ax[nn - 1 - ix, iy].imshow(epsf, norm=norm)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "else:\n",
    "    \n",
    "    plt.figure(figsize = (14, 14))\n",
    "    nn = 2 \n",
    "    for i in range(nn):\n",
    "        ax = plt.subplot(1, nn, i + 1)\n",
    "        \n",
    "        epsf = epsf_tot[i].data\n",
    "        \n",
    "        ax.set_xlabel('X [px]')\n",
    "        ax.set_ylabel('Y [px]')\n",
    "        ax.set_title(det + ' - ' + filt +  ' - ePSF' + str(i+1), fontsize=20)\n",
    "        norm = simple_norm(epsf, 'sqrt', percent=99.)\n",
    "        \n",
    "        ax.imshow(epsf, norm=norm)\n",
    "       \n",
    "        plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef398d9",
   "metadata": {},
   "source": [
    "5.<font color='white'>-</font>Create a single or grid of empirical PSFs <a class=\"anchor\" id=\"eps_intro2\"></a>\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a69d66a",
   "metadata": {},
   "source": [
    "### 5.1<font color='white'>-</font>Count stars in N x N grid<a class=\"anchor\" id=\"count_stars\"></a> ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64e7a92",
   "metadata": {},
   "source": [
    "The purpose of the function count_stars_grid is to count how many good PSF stars are in cell of a N x N grid. The function starts from a grid of size N x N (where N = sqrt(num_psfs)) and iterate until the minimum grid size 2 x 2. Depending on the number of PSF stars that the users want in each cell of the grid, they can choose the appropriate grid size or modify the threshold values and/or the selection parameters adopted during the stars detection, in Sections 4.3, 4.4.\n",
    "\n",
    "The minimum number of PSF stars needed in each cell can also be set using the parameter min_numpsfs_stars. Useful when inspecting the plot, since in the cells with a number of PSF stars < min_numpsfs_stars, the value is reported in RED. Moreover, when verbose = True, it is easier to identify for each N x N combination, if and which cells have not enough PSF stars.\n",
    "\n",
    "This function returns sqrt(num_psfs) - 1 figures showing the number of PSFs stars in each cell for all the N x N combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67799bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_centers(num):\n",
    "    points = int(((data.shape[0] / num) / 2) - 1)\n",
    "    x_center = np.arange(points, 2 * points * num, 2 * points)\n",
    "    y_center = np.arange(points, 2 * points * num, 2 * points)\n",
    "\n",
    "    centers = np.array(np.meshgrid(x_center, y_center)).T.reshape(-1, 2)\n",
    "\n",
    "    return points, centers\n",
    "\n",
    "def count_stars_grid(table, data, num_psfs=4, min_numpsf_stars=40, size=11, verbose=True, savefig=True):\n",
    "    \n",
    "    # calculate the number of stars from find_stars in each cell of the grid. The maximum number of cell\n",
    "    # is defined by num_psfs and the function iterate from N x N (where N = sqrt(num_psfs)) until a 2 x 2 grid.\n",
    "\n",
    "    if np.sqrt(num_psfs).is_integer():\n",
    "        grid_points = int(np.sqrt(num_psfs))\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"You must choose a square number of cells to create (E.g. 9, 16, etc.)\")\n",
    "\n",
    "    num_grid = np.arange(2, grid_points + 1, 1)\n",
    "    num_grid = num_grid[::-1]\n",
    "\n",
    "\n",
    "    for num in num_grid:\n",
    "        print(\"--------------------\")\n",
    "        print(\"\")\n",
    "        print(\"Calculating the number of PSF stars in a %d x %d grid:\" % (num, num))\n",
    "        print(\"\")\n",
    "\n",
    "        s = (data.shape[1], data.shape[0])\n",
    "        temp_arr = np.zeros(s)\n",
    "        num_psfs_stars = []\n",
    "\n",
    "        points, centers = find_centers(num)\n",
    "\n",
    "        for n, val in enumerate(centers):\n",
    "\n",
    "            x = table['xcentroid']\n",
    "            y = table['ycentroid']\n",
    "\n",
    "            half_size = (size - 1) / 2\n",
    "\n",
    "            lim1 = int(val[0] - points + half_size)\n",
    "            lim2 = int(val[0] + points - half_size)\n",
    "            lim3 = int(val[1] - points + half_size)\n",
    "            lim4 = int(val[1] + points - half_size)\n",
    "\n",
    "            number_psf_stars = (x > lim1) & (x < lim2) & (y > lim3) & (y < lim4)\n",
    "            count_psfs_stars = np.count_nonzero(number_psf_stars)\n",
    "\n",
    "            lim_x1 = int(lim1 - half_size)\n",
    "            lim_x2 = int(lim2 + half_size)\n",
    "            lim_y1 = int(lim3 - half_size)\n",
    "            lim_y2 = int(lim4 + half_size)\n",
    "\n",
    "            if verbose:\n",
    "\n",
    "                if np.count_nonzero(number_psf_stars) < min_numpsf_stars:\n",
    "                    print('Center Coordinates of grid cell {:d} are ({:d}, {:d}) --- Not enough stars in the cell '\n",
    "                            '(number of stars < {:d})'.format(n + 1, val[0], val[1], min_numpsf_stars))\n",
    "\n",
    "                else:\n",
    "                    print(f'Center Coordinate of grid cell {n + 1:d} are ({val[0]:d}, {val[1]:d})'\n",
    "                            '--- Number of stars:', np.count_nonzero(number_psf_stars))\n",
    "                    print(\"\")\n",
    "\n",
    "            temp_arr[lim_y1:lim_y2, lim_x1:lim_x2] = count_psfs_stars\n",
    "            num_psfs_stars.append(count_psfs_stars)\n",
    "\n",
    "        if savefig:\n",
    "            plot_count_grid(temp_arr, num, num_psfs_stars, centers)\n",
    "\n",
    "def plot_count_grid(arr, num, nstars, centers):\n",
    "    \n",
    "    plt.clf()\n",
    "    \n",
    "    from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    ax = plt.subplot(1, 1, 1)\n",
    "\n",
    "    plt.xlabel('X [px]', font={'size': 20})\n",
    "    plt.ylabel('Y [px]', font={'size': 20})\n",
    "    plt.title('%dx%d grid - ' % (num, num) + det + ' - ' + filt, font={'size': 25})\n",
    "    im = ax.imshow(arr, origin='lower', vmin=np.min(arr[arr > 0]), vmax=np.max(arr))\n",
    "    for i in range(num ** 2):\n",
    "        if nstars[i] < 40:\n",
    "            ax.text(centers[i][0] - 100, centers[i][1] - 50, \"%d\" % nstars[i], c='r', font={'size': 30})\n",
    "        else:\n",
    "            ax.text(centers[i][0] - 100, centers[i][1] - 50, \"%d\" % nstars[i], c='w', font={'size': 30})\n",
    "    ax.text(2300, 750, \"# of PSF stars\", rotation=270, font={'size': 25})\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im, cax=cax)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    filename = 'number_PSFstars_{}x{}grid_fov{}_{}_{}_image_{}.pdf'.format(num, num, size, det, filt, nimage)\n",
    "\n",
    "    plt.savefig(os.path.join(figures_dir, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdacf819",
   "metadata": {},
   "outputs": [],
   "source": [
    "figures_dir = 'FIGURES/'\n",
    "\n",
    "if not os.path.exists(figures_dir):\n",
    "    os.makedirs(figures_dir)\n",
    "\n",
    "for i, (data, table) in enumerate(zip(data_bkgsub_tot, found_stars_sel_tot)):\n",
    "    \n",
    "    nimage = str(i+1)\n",
    "    size = 11\n",
    "    \n",
    "    count_stars_grid(table, data, num_psfs=25, min_numpsf_stars=40, size=size, verbose=True, savefig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a192ed7",
   "metadata": {},
   "source": [
    "### 5.2<font color='white'>-</font>Build effective PSF (single or grid)<a class=\"anchor\" id=\"epsf_grid\"></a> ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1b206d",
   "metadata": {},
   "source": [
    "This function creates a grid of PSFs with EPSFBuilder (or a single PSF, when **num_psfs**=1). The function returns a GriddedEPSFModel object containing a 3D array of N  ×  n  ×  n. The 3D array represents the N number of 2D n  ×  n ePSFs created. It includes a grid_xypos key which will state the position of the PSF on the detector for each of the PSFs. The order of the tuples in grid_xypos refers to the number the PSF is in the 3D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bc0ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_epsf_grid(table, data, num_psfs=4, size=11, oversample=4, save=True, savefig=True, overwrite=True):\n",
    "\n",
    "    if np.sqrt(num_psfs).is_integer():\n",
    "        num_grid = int(np.sqrt(num_psfs))\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"You must choose a square number of cells to create (E.g. 9, 16, etc.)\")\n",
    "\n",
    "\n",
    "    points, centers = find_centers(num_grid)\n",
    "\n",
    "    epsf_size = size * oversample\n",
    "    epsf_arr = np.empty((num_grid ** 2, epsf_size + 1, epsf_size + 1))\n",
    "\n",
    "    for i, val in enumerate(centers):\n",
    "\n",
    "        x = table['xcentroid']\n",
    "        y = table['ycentroid']\n",
    "\n",
    "        half_size = (size - 1) / 2\n",
    "\n",
    "        lim1 = int(val[0] - points + half_size)\n",
    "        lim2 = int(val[0] + points - half_size)\n",
    "        lim3 = int(val[1] - points + half_size)\n",
    "        lim4 = int(val[1] + points - half_size)\n",
    "\n",
    "        mask = ((x > lim1) & (x < lim2) & (y > lim3) & (y < lim4))\n",
    "\n",
    "        stars_tbl = Table()\n",
    "        stars_tbl['x'] = x[mask]\n",
    "        stars_tbl['y'] = y[mask]\n",
    "        print('Number of sources in cell %d used to build the ePSF:' % (i + 1), len(stars_tbl['x']))\n",
    "\n",
    "        nddata = NDData(data=data)\n",
    "        stars = extract_stars(nddata, stars_tbl, size=size)\n",
    "\n",
    "        print(\"Creating ePSF for cell %d - Coordinates (%d, %d)\" % (i + 1, val[0], val[1]))\n",
    "        print(\"\")\n",
    "\n",
    "        epsf_builder = EPSFBuilder(oversampling=oversample, maxiters=3, progress_bar=False)\n",
    "\n",
    "        epsf, fitted_stars = epsf_builder(stars)\n",
    "\n",
    "        epsf_arr[i, :, :] = epsf.data\n",
    "\n",
    "        meta = OrderedDict()\n",
    "        meta[\"DETECTOR\"] = (det, \"Detector name\")\n",
    "        meta[\"FILTER\"] = (filt, \"Filter name\")\n",
    "        meta[\"NUM_PSFS\"] = (num_grid ** 2, \"The total number of ePSFs\")\n",
    "        for h, loc in enumerate(centers):\n",
    "            loc = np.asarray(loc, dtype=float)\n",
    "\n",
    "            meta[\"DET_YX{}\".format(h)] = (str((loc[1], loc[0])),\n",
    "                                            \"The #{} PSF's (y,x) detector pixel position\".format(h))\n",
    "\n",
    "        meta[\"OVERSAMP\"] = (oversample, \"Oversampling Factor in EPSFBuilder\")\n",
    "\n",
    "        model_epsf = create_model(epsf_arr, meta)\n",
    "\n",
    "    if savefig:\n",
    "        plot_epsf(model_epsf, num_psfs)\n",
    "\n",
    "    if save:\n",
    "        writeto(epsf_arr, meta, num_psfs)\n",
    "\n",
    "        return model_epsf\n",
    "\n",
    "def writeto(data, meta, num_psfs, overwrite=True):\n",
    "\n",
    "    primaryhdu = fits.PrimaryHDU(data)\n",
    "    \n",
    "    # Convert meta dictionary to header\n",
    "    tuples = [(a, b, c) for (a, (b, c)) in meta.items()]\n",
    "    primaryhdu.header.extend(tuples)\n",
    "\n",
    "    # Add extra descriptors for how the file was made\n",
    "    primaryhdu.header[\"COMMENT\"] = \"For a given filter, and detector 1 file is produced in \"\n",
    "    primaryhdu.header[\"COMMENT\"] = \"the form [i, y, x] where i is the ePSF position on the detector grid \"\n",
    "    primaryhdu.header[\"COMMENT\"] = \"and (y,x) is the 2D PSF. The order of PSFs can be found under the \"\n",
    "    primaryhdu.header[\"COMMENT\"] = \"header DET_YX* keywords\"\n",
    "\n",
    "    hdu = fits.HDUList(primaryhdu)\n",
    "\n",
    "    filename = \"ePSF_{}_{}_fov{}_nepsf{}_image_{}.fits\".format(det, filt, size, num_psfs, nimage)\n",
    "    \n",
    "    file = os.path.join(epsf_dir, filename)\n",
    "\n",
    "    hdu.writeto(file, overwrite=overwrite)\n",
    "\n",
    "def plot_epsf(model, num):\n",
    "\n",
    "    if num == 1:\n",
    "        plt.clf()\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        ax = plt.subplot(1, 1, 1)\n",
    "\n",
    "        norm_epsf = simple_norm(model.data[0], 'log', percent=99.)\n",
    "        plt.suptitle(det + ' - ' + filt, font={'size': 20})\n",
    "        plt.title(model.meta['grid_xypos'][0], font={'size': 20})\n",
    "        ax.imshow(model.data[0], norm=norm_epsf)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        filename = 'ePSF_single_{}_{}_fov{}_image_{}.pdf'.format(det, filt, size, nimage)\n",
    "        \n",
    "        plt.savefig(os.path.join(figures_dir, filename))\n",
    " \n",
    "    else:\n",
    "        plt.clf()\n",
    "\n",
    "        nn = int(np.sqrt(num))\n",
    "        figsize = (12, 12)\n",
    "        fig, ax = plt.subplots(nn, nn, figsize=figsize)\n",
    "\n",
    "        for ix in range(nn):\n",
    "            for iy in range(nn):\n",
    "                i = ix * nn + iy\n",
    "                norm_epsf = simple_norm(model.data[i], 'log', percent=99.)\n",
    "                ax[nn - 1 - iy, ix].imshow(model.data[i], norm=norm_epsf)\n",
    "                ax[nn - 1 - iy, ix].set_title(model.meta['grid_xypos'][i], font={'size': 20})\n",
    "\n",
    "        plt.suptitle(det + ' - ' + filt, font={'size': 40})\n",
    "        plt.tight_layout()\n",
    "    \n",
    "        filename = 'ePSF_{}x{}grid_{}_{}_fov{}_image_{}.pdf'.format(nn, nn, det, filt, size, nimage)\n",
    "        \n",
    "        plt.savefig(os.path.join(figures_dir, filename))\n",
    "\n",
    "\n",
    "def create_model(data, meta):\n",
    "\n",
    "    ndd = NDData(data, meta=meta, copy=True)\n",
    "\n",
    "    ndd.meta['grid_xypos'] = [((float(ndd.meta[key][0].split(',')[1].split(')')[0])),\n",
    "                                   (float(ndd.meta[key][0].split(',')[0].split('(')[1]))) for key in ndd.meta.keys() if\n",
    "                                  \"DET_YX\" in key]\n",
    "\n",
    "    ndd.meta['oversampling'] = meta[\"OVERSAMP\"][0]\n",
    "    ndd.meta = {key.lower(): ndd.meta[key] for key in ndd.meta}\n",
    "    model = GriddedPSFModel(ndd)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad1c315",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsf_grid_tot = []\n",
    "\n",
    "distorted = True\n",
    "size = 41\n",
    "num_psf = 4\n",
    "oversample = 4\n",
    "\n",
    "if distorted:\n",
    "    epsf_dir = 'ePSF_MODELS/Distorted/Grid/Fov{}px_numPSFs{}_oversample{}'.format(size, num_psf, oversample)\n",
    "\n",
    "    if not os.path.exists(epsf_dir):\n",
    "        os.makedirs(epsf_dir)\n",
    "else:\n",
    "    epsf_dir = 'ePSF_MODELS/Undistorted/Grid/Fov{}px_numPSFs{}_oversample{}'.format(size, num_psf, oversample)\n",
    "    \n",
    "    if not os.path.exists(epsf_dir):\n",
    "        os.makedirs(epsf_dir)\n",
    "\n",
    "        \n",
    "for i, (data, table) in enumerate(zip(data_bkgsub_tot, found_stars_sel_tot)):\n",
    "    \n",
    "    nimage = str(i+1)\n",
    "    \n",
    "    print('----------------')\n",
    "    print('Working on image {}'.format(str(i+1)))\n",
    "    print('----------------')\n",
    "\n",
    "    epsf_grid = build_epsf_grid(table, data, num_psfs=num_psf, size=size, oversample=oversample, save=True, \n",
    "                                savefig=True, overwrite=True)\n",
    "    epsf_grid_tot.append(epsf_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d832133b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
